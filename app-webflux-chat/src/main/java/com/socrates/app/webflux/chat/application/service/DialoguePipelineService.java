package com.socrates.app.webflux.chat.application.service;

import com.socrates.app.webflux.chat.application.monitoring.DialoguePipelineMonitor;
import com.socrates.app.webflux.chat.application.monitoring.DialoguePipelineStage;
import com.socrates.app.webflux.chat.application.monitoring.DialoguePipelineTracker;
import com.socrates.app.webflux.chat.domain.model.conversation.ConversationContext;
import com.socrates.app.webflux.chat.domain.model.conversation.ConversationTurn;
import com.socrates.app.webflux.chat.domain.model.llm.CompletionRequest;
import com.socrates.app.webflux.chat.domain.model.llm.Message;
import com.socrates.app.webflux.chat.domain.model.memory.MemoryRetrievalResult;
import com.socrates.app.webflux.chat.domain.model.rag.RetrievalContext;
import com.socrates.app.webflux.chat.domain.port.in.DialoguePipelineUseCase;
import com.socrates.app.webflux.chat.domain.port.out.ConversationCounterPort;
import com.socrates.app.webflux.chat.domain.port.out.ConversationRepository;
import com.socrates.app.webflux.chat.domain.port.out.LlmPort;
import com.socrates.app.webflux.chat.domain.port.out.PromptTemplatePort;
import com.socrates.app.webflux.chat.domain.port.out.RetrievalPort;
import com.socrates.app.webflux.chat.domain.port.out.TtsPort;
import com.socrates.app.webflux.chat.domain.service.SentenceAssembler;
import java.util.ArrayList;
import java.util.Base64;
import java.util.List;
import java.util.stream.Collectors;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.core.scheduler.Schedulers;

@Slf4j
@Service
public class DialoguePipelineService implements DialoguePipelineUseCase {

	private final LlmPort llmPort;
	private final TtsPort ttsPort;
	private final RetrievalPort retrievalPort;
	private final ConversationRepository conversationRepository;
	private final SentenceAssembler sentenceAssembler;
	private final DialoguePipelineMonitor pipelineMonitor;
	private final ConversationCounterPort conversationCounterPort;
	private final MemoryExtractionService memoryExtractionService;

	private final PromptTemplatePort promptTemplatePort;

	private final int conversationThreshold; // 대화 횟수 임계값

	public DialoguePipelineService(LlmPort llmPort,
		TtsPort ttsPort,
		RetrievalPort retrievalPort,
		ConversationRepository conversationRepository,
		SentenceAssembler sentenceAssembler,
		DialoguePipelineMonitor pipelineMonitor,
		ConversationCounterPort conversationCounterPort,
		MemoryExtractionService memoryExtractionService,
		PromptTemplatePort promptTemplatePort,
		int conversationThreshold) {
		this.llmPort = llmPort;
		this.ttsPort = ttsPort;
		this.retrievalPort = retrievalPort;
		this.conversationRepository = conversationRepository;
		this.sentenceAssembler = sentenceAssembler;
		this.pipelineMonitor = pipelineMonitor;
		this.conversationCounterPort = conversationCounterPort;
		this.memoryExtractionService = memoryExtractionService;
		this.promptTemplatePort = promptTemplatePort;
		this.conversationThreshold = conversationThreshold;
	}

	/**
	 * 대화 파이프라인을 실행하고 텍스트 스트림을 반환합니다.
	 *
	 * @param text
	 *            대화 텍스트
	 * @return 텍스트 스트림
	 */
	@Override
	public Flux<String> executeStreaming(String text) {
		return executeAudioStreaming(text).map(bytes -> Base64.getEncoder().encodeToString(bytes));
	}

	@Override
	public Flux<String> executeTextOnly(String text) {
		DialoguePipelineTracker tracker = pipelineMonitor.create(text);

		String systemPrompt = promptTemplatePort.buildDefaultPrompt();

		List<Message> messages = new ArrayList<>();
		messages.add(Message.system(systemPrompt));
		messages.add(Message.user(text));

		CompletionRequest request = CompletionRequest.withMessages(messages, "gpt-4o-mini", true);

		tracker.recordStageAttribute(DialoguePipelineStage.LLM_COMPLETION, "model", request.model());

		Flux<String> llmTokens = tracker.traceFlux(DialoguePipelineStage.LLM_COMPLETION,
			() -> llmPort.streamCompletion(request))
			.doOnNext(token -> tracker
				.incrementStageCounter(DialoguePipelineStage.LLM_COMPLETION, "tokenCount", 1))
			.subscribeOn(Schedulers.boundedElastic());

		Flux<String> textStream = llmTokens.doOnNext(token -> tracker.markResponseEmission());

		return tracker.attachLifecycle(textStream);
	}

	/**
	 * 대화 파이프라인을 실행하고 오디오 스트림을 반환합니다.
	 *
	 * @param text
	 *            대화 텍스트
	 * @return 오디오 스트림
	 */
	@Override
	public Flux<byte[]> executeAudioStreaming(String text) {
		DialoguePipelineTracker tracker = pipelineMonitor.create(text);

		// TTS 준비
		Mono<Void> ttsWarmup = tracker.traceMono(DialoguePipelineStage.TTS_PREPARATION,
			() -> ttsPort.prepare()
				.doOnError(error -> log
					.warn("파이프라인 {}의 TTS 준비 실패: {}", tracker.pipelineId(), error.getMessage()))
				.onErrorResume(error -> Mono.empty()))
			.cache();

		ttsWarmup.subscribe();

		String systemPrompt = promptTemplatePort.buildDefaultPrompt();

		List<Message> messages = new ArrayList<>();
		messages.add(Message.system(systemPrompt));
		messages.add(Message.user(text));

		CompletionRequest request = CompletionRequest.withMessages(messages, "gpt-4o-mini", false);

		tracker.recordStageAttribute(DialoguePipelineStage.LLM_COMPLETION, "model", request.model());

		Mono<String> llmResponse = tracker.traceMono(DialoguePipelineStage.LLM_COMPLETION,
			() -> llmPort.complete(request))
			.doOnNext(response -> {
				tracker.incrementStageCounter(DialoguePipelineStage.LLM_COMPLETION,
					"tokenCount",
					1);
				tracker.recordLlmOutput(response);
				tracker.markResponseEmission();
			}).subscribeOn(Schedulers.boundedElastic());

		Flux<byte[]> audioStream = llmResponse.flatMapMany(response -> tracker
			.traceFlux(DialoguePipelineStage.TTS_SYNTHESIS,
				() -> ttsWarmup.thenMany(ttsPort.streamSynthesize(response)))
			.doOnNext(chunk -> tracker.incrementStageCounter(DialoguePipelineStage.TTS_SYNTHESIS,
				"audioChunks",
				1)));

		return tracker.attachLifecycle(audioStream);
	}

	/**
	 * 쿼리를 저장하고 ConversationTurn을 반환합니다.
	 *
	 * @param text
	 *            쿼리
	 * @return ConversationTurn
	 */
	private Mono<ConversationTurn> saveQuery(String text) {
		ConversationTurn turn = ConversationTurn.create(text);
		return conversationRepository.save(turn);
	}

	/**
	 * 대화 기록을 로드하고 ConversationContext를 반환합니다.
	 */
	private Mono<ConversationContext> loadConversationHistory() {
		return conversationRepository.findRecent(10).collectList().map(ConversationContext::of)
			.defaultIfEmpty(ConversationContext.empty());
	}

	/**
	 * 메시지를 생성하고 List<Message>를 반환합니다.
	 */
	private List<Message> buildMessages(RetrievalContext context,
		MemoryRetrievalResult memories,
		ConversationContext conversationContext,
		String currentQuery) {
		List<Message> messages = new ArrayList<>();

		String systemPrompt = buildSystemPrompt(context, memories);
		messages.add(Message.system(systemPrompt));

		conversationContext.turns().stream().filter(turn -> turn.response() != null)
			.forEach(turn -> {
				messages.add(Message.user(turn.query()));
				messages.add(Message.assistant(turn.response()));
			});

		messages.add(Message.user(currentQuery));

		return messages;
	}

	/**
	 * 시스템 프롬프트를 생성하고 String을 반환합니다.
	 */
	private String buildSystemPrompt(RetrievalContext context, MemoryRetrievalResult memories) {
		StringBuilder prompt = new StringBuilder();

		prompt.append("자연스럽게 대화하세요. 과도한 존댓말이나 '도와드리겠습니다' 같은 틀에 박힌 표현은 피하세요.\n\n");

		if (!memories.isEmpty()) {
			prompt.append("대화 상대에 대한 기억:\n");

			if (!memories.experientialMemories().isEmpty()) {
				prompt.append("\n경험적 기억:\n");
				memories.experientialMemories()
					.forEach(m -> prompt.append("- ").append(m.content()).append("\n"));
			}

			if (!memories.factualMemories().isEmpty()) {
				prompt.append("\n사실 기반 기억:\n");
				memories.factualMemories()
					.forEach(m -> prompt.append("- ").append(m.content()).append("\n"));
			}

			prompt.append("\n");
		}

		if (!context.isEmpty()) {
			String contextText = context.documents().stream().map(doc -> doc.content())
				.collect(Collectors.joining("\n"));
			prompt.append("참고 정보:\n").append(contextText).append("\n\n");
		}

		return prompt.toString();
	}
}
